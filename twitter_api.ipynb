{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Required packages\n",
    "import tweepy\n",
    "import argparse\n",
    "import json\n",
    "import ijson\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy\n",
    "#set up working directory\n",
    "os.getcwd()\n",
    "os.chdir(\"/Users/nicolekelly/Documents/ft_aggregate/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setting up Twitter API\n",
    "#twitterkeys.py is a file that contains my consumer_key, consumer_secret, access_token and access_token_secret\n",
    "import twitterkeys\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitterkeys.consumer_key, twitterkeys.consumer_secret)\n",
    "auth.set_access_token(twitterkeys.access_token, twitterkeys.access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#second method from David\n",
    "#Pull in most recent 20 tweets\n",
    "ArepaZone_timeline=api.user_timeline(\"ArepaZone\")\n",
    "arepa_tweets=[]\n",
    "for tweet in ArepaZone_timeline:\n",
    "    tweet_parts = {\n",
    "        \"id\":tweet.id,\n",
    "        \"created_at\":tweet.created_at,\n",
    "        \"text\":tweet.text\n",
    "    }\n",
    "    arepa_tweets.append(tweet_parts)\n",
    "\n",
    "ArepaZone_df = pd.DataFrame(arepa_tweets)\n",
    "ArepaZone_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-27-b6d25898c3cb>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-b6d25898c3cb>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    return food_truck_twitter_handle_df\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "#doesn't work yet, something with the pd.DataFrame line syntax\n",
    "trucks=['captaincookiedc']\n",
    "for food_truck_twitter_handle in trucks:\n",
    "    food_truck_twitter_handle_timeline=api.user_timeline('{}'.format(food_truck_twitter_handle))\n",
    "    food_truck_twitter_handle_tweets=[]\n",
    "    for tweet in food_truck_twitter_handle_timeline:\n",
    "        tweet_parts = {\n",
    "            \"id\":tweet.id,\n",
    "            \"created_at\":tweet.created_at,\n",
    "            \"text\":tweet.text\n",
    "        }\n",
    "        food_truck_twitter_handle_tweets.append(tweet_parts)\n",
    "    food_truck_twitter_handle_df = pd.DataFrame(food_truck_twitter_handle_tweets)\n",
    "    return food_truck_twitter_handle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doesn't work yet, something with the pd.DataFrame line syntax\n",
    "def tweets_to_dataframe(food_truck_twitter_handle, food_truck_twitter_handle_string):\n",
    "    food_truck_twitter_handle_timeline=api.user_timeline('{}'.format(food_truck_twitter_handle_string))\n",
    "    food_truck_twitter_handle_tweets=[]\n",
    "    for tweet in food_truck_twitter_handle_timeline:\n",
    "        tweet_parts = {\n",
    "            \"id\":tweet.id,\n",
    "            \"created_at\":tweet.created_at,\n",
    "            \"text\":tweet.text\n",
    "        }\n",
    "        food_truck_twitter_handle_tweets.append(tweet_parts)\n",
    "    food_truck_twitter_handle_df = pd.DataFrame(food_truck_twitter_handle_string_tweets)\n",
    "    return food_truck_twitter_handle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'food_truck_twitter_handle_string_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7bae20c77f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'captaincookiedc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'captaincookiedc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-7ff859a2e9aa>\u001b[0m in \u001b[0;36mtweets_to_dataframe\u001b[0;34m(food_truck_twitter_handle, food_truck_twitter_handle_string)\u001b[0m\n\u001b[1;32m      9\u001b[0m         }\n\u001b[1;32m     10\u001b[0m         \u001b[0mfood_truck_twitter_handle_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfood_truck_twitter_handle_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfood_truck_twitter_handle_string_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfood_truck_twitter_handle_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'food_truck_twitter_handle_string_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_to_dataframe('captaincookiedc','captaincookiedc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pull in most recent 20 tweets\n",
    "ArepaZone_timeline=api.user_timeline(\"ArepaZone\")\n",
    "#Set a number to loop through\n",
    "i=-1\n",
    "#Create arrays for each feature of the timeline json object to extract\n",
    "ArepaZone_id_array=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "ArepaZone_date_array=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "ArepaZone_tweet_array=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "#Loop through the tweet objects and add the feature contents to the relevant arrays\n",
    "for tweet in ArepaZone_timeline:\n",
    "    i=i+1\n",
    "    ArepaZone_id_array[i]=tweet.id\n",
    "    ArepaZone_date_array[i]=tweet.created_at\n",
    "    ArepaZone_tweet_array[i]=tweet.text\n",
    "\n",
    "\n",
    "#Create the pandas dataframe using the ID array to merge the other features into\n",
    "ArepaZone_merged_arrays_df = pd.DataFrame(data=ArepaZone_id_array, columns=['id']) \n",
    "\n",
    "\n",
    "#Create dataframes for the other features so they are able to be merged\n",
    "ArepaZone_tweet_array_df = pd.DataFrame(data=ArepaZone_tweet_array, columns=['tweet']) \n",
    "ArepaZone_date_array_df = pd.DataFrame(data=ArepaZone_date_array, columns=['date']) \n",
    "\n",
    "\n",
    "#Merge the other feature dataframes into the main dataframe\n",
    "ArepaZone_merged_arrays_df=ArepaZone_merged_arrays_df.merge(ArepaZone_tweet_array_df,left_index=True, right_index=True)\n",
    "ArepaZone_merged_arrays_df=ArepaZone_merged_arrays_df.merge(ArepaZone_date_array_df,left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#Output the dataframe to a text file to save it (will eventually want to do this in SQL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These are functions of all of these commands above\n",
    "def tweets_to_dataframe(food_truck_twitter_handle, food_truck_twitter_handle_string):\n",
    "    food_truck_twitter_handle_timeline=\"\"\n",
    "    food_truck_twitter_handle_timeline=api.user_timeline('{}'.format(food_truck_twitter_handle_string))\n",
    "    #Set a number to loop through\n",
    "    i=-1\n",
    "    #Create arrays for each feature of the timeline json object to extract\n",
    "    food_truck_twitter_handle_id_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle_string)),dtype=object)\n",
    "    food_truck_twitter_handle_date_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle_string)),dtype=object)\n",
    "    food_truck_twitter_handle_tweet_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle_string)),dtype=object)\n",
    "    #Loop through the tweet objects and add the feature contents to the relevant arrays\n",
    "    for tweet in food_truck_twitter_handle_timeline:\n",
    "        i=i+1\n",
    "        food_truck_twitter_handle_id_array[i]=tweet.id\n",
    "        food_truck_twitter_handle_date_array[i]=tweet.created_at\n",
    "        food_truck_twitter_handle_tweet_array[i]=tweet.text\n",
    "    #Create the pandas dataframe using the ID array to merge the other features into\n",
    "    food_truck_twitter_handle_merged_arrays_df = pd.DataFrame(data=food_truck_twitter_handle_id_array, columns=['id']) \n",
    "    #food_truck_twitter_handle_merged_arrays_df = pd.DataFrame(data=eval('{}_id_array'.format(food_truck_twitter_handle_string)), columns=['id']) \n",
    "    #Create dataframes for the other features so they are able to be merged\n",
    "    food_truck_twitter_handle_tweet_array_df = pd.DataFrame(data=food_truck_twitter_handle_tweet_array, columns=['tweet']) \n",
    "    food_truck_twitter_handle_date_array_df = pd.DataFrame(data=food_truck_twitter_handle_date_array, columns=['date']) \n",
    "    #Merge the other feature dataframes into the main dataframe\n",
    "    food_truck_twitter_handle_merged_arrays_df=food_truck_twitter_handle_merged_arrays_df.merge(food_truck_twitter_handle_tweet_array_df,left_index=True, right_index=True)\n",
    "    food_truck_twitter_handle_merged_arrays_df=food_truck_twitter_handle_merged_arrays_df.merge(food_truck_twitter_handle_date_array_df,left_index=True, right_index=True)\n",
    "    log_tweets = \"/Users/nicolekelly/Documents/ft_aggregate/\" + str(time.time()) + '_' + '{}'.format(food_truck_twitter_handle_string)+'.csv'\n",
    "    food_truck_twitter_handle_merged_arrays_df.to_csv(path_or_buf=log_tweets, header=['ID','Tweet','Date'], index=True, sep=',')\n",
    "    return food_truck_twitter_handle_merged_arrays_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 18 is out of bounds for axis 0 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-85bcffc488f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#abunai_merged_arrays_df=tweets_to_dataframe('abunaifood', 'abunaifood')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mballornothing_merged_arrays_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweets_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'theballtruck'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'theballtruck'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0marepazone_merged_arrays_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweets_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ArepaZone'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ArepaZone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-d6225f553ad2>\u001b[0m in \u001b[0;36mtweets_to_dataframe\u001b[0;34m(food_truck_twitter_handle, food_truck_twitter_handle_string)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfood_truck_twitter_handle_timeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfood_truck_twitter_handle_id_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mfood_truck_twitter_handle_date_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfood_truck_twitter_handle_tweet_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18 is out of bounds for axis 0 with size 18"
     ]
    }
   ],
   "source": [
    "healthyfool_merged_arrays_df=tweets_to_dataframe('healthyfool', 'healthyfool')\n",
    "#abunai_merged_arrays_df=tweets_to_dataframe('abunaifood', 'abunaifood')\n",
    "ballornothing_merged_arrays_df=tweets_to_dataframe('theballtruck', 'theballtruck')\n",
    "arepazone_merged_arrays_df=tweets_to_dataframe('ArepaZone','ArepaZone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>817029652392931330</td>\n",
       "      <td>Farragut Square today ðŸ™Œ https://t.co/zoxKGV2XXR</td>\n",
       "      <td>2017-01-05 15:27:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>816749041182801921</td>\n",
       "      <td>Looking for dinner? Our Union Market stall is ...</td>\n",
       "      <td>2017-01-04 20:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>816566606050500608</td>\n",
       "      <td>@magneticdynamo not this week, but we are next...</td>\n",
       "      <td>2017-01-04 08:47:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>816430441242750976</td>\n",
       "      <td>Tomorrow's food truck spot is 19th and L, NW! ...</td>\n",
       "      <td>2017-01-03 23:46:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>816386654659366913</td>\n",
       "      <td>Thanks for joining us for lunch! For those who...</td>\n",
       "      <td>2017-01-03 20:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>816305643900596224</td>\n",
       "      <td>Getting ready to open our window at Franklin S...</td>\n",
       "      <td>2017-01-03 15:30:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>816262061013475328</td>\n",
       "      <td>Happy New Year! We're super excited for this y...</td>\n",
       "      <td>2017-01-03 12:36:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>814855551054204928</td>\n",
       "      <td>Catch us one last time this year! We're parked...</td>\n",
       "      <td>2016-12-30 15:27:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>814541106566365184</td>\n",
       "      <td>@Debbliss99 solo por encargo! LlÃ¡manos al 7032...</td>\n",
       "      <td>2016-12-29 18:38:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>814476576838586369</td>\n",
       "      <td>We are headed to Farragut Square one last time...</td>\n",
       "      <td>2016-12-29 14:22:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>813798747645808640</td>\n",
       "      <td>Metro Center today!</td>\n",
       "      <td>2016-12-27 17:28:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>811935966445404160</td>\n",
       "      <td>Farragut Square today! âœŒ</td>\n",
       "      <td>2016-12-22 14:06:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>811598839845101568</td>\n",
       "      <td>Getting ready to open at 19th and L!</td>\n",
       "      <td>2016-12-21 15:46:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>811238039108415488</td>\n",
       "      <td>Hello, Metro Center! Come have an arepa with u...</td>\n",
       "      <td>2016-12-20 15:53:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>810859034576351232</td>\n",
       "      <td>Headed to the World Bank headquarters today!</td>\n",
       "      <td>2016-12-19 14:47:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>809410266714800128</td>\n",
       "      <td>The cold can't stop us! We will be serving lun...</td>\n",
       "      <td>2016-12-15 14:50:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>809082765031469056</td>\n",
       "      <td>Open until 2pm at 19th and L! https://t.co/T7v...</td>\n",
       "      <td>2016-12-14 17:08:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>808690035377799169</td>\n",
       "      <td>Good morning, Metro Center! We hope you're hun...</td>\n",
       "      <td>2016-12-13 15:08:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>808338419181031424</td>\n",
       "      <td>Happy Monday NoMa! Getting ready to open our w...</td>\n",
       "      <td>2016-12-12 15:51:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>806893320588902400</td>\n",
       "      <td>Getting ready to open up our window at Farragu...</td>\n",
       "      <td>2016-12-08 16:08:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0   817029652392931330    Farragut Square today ðŸ™Œ https://t.co/zoxKGV2XXR   \n",
       "1   816749041182801921  Looking for dinner? Our Union Market stall is ...   \n",
       "2   816566606050500608  @magneticdynamo not this week, but we are next...   \n",
       "3   816430441242750976  Tomorrow's food truck spot is 19th and L, NW! ...   \n",
       "4   816386654659366913  Thanks for joining us for lunch! For those who...   \n",
       "5   816305643900596224  Getting ready to open our window at Franklin S...   \n",
       "6   816262061013475328  Happy New Year! We're super excited for this y...   \n",
       "7   814855551054204928  Catch us one last time this year! We're parked...   \n",
       "8   814541106566365184  @Debbliss99 solo por encargo! LlÃ¡manos al 7032...   \n",
       "9   814476576838586369  We are headed to Farragut Square one last time...   \n",
       "10  813798747645808640                                Metro Center today!   \n",
       "11  811935966445404160                           Farragut Square today! âœŒ   \n",
       "12  811598839845101568               Getting ready to open at 19th and L!   \n",
       "13  811238039108415488  Hello, Metro Center! Come have an arepa with u...   \n",
       "14  810859034576351232       Headed to the World Bank headquarters today!   \n",
       "15  809410266714800128  The cold can't stop us! We will be serving lun...   \n",
       "16  809082765031469056  Open until 2pm at 19th and L! https://t.co/T7v...   \n",
       "17  808690035377799169  Good morning, Metro Center! We hope you're hun...   \n",
       "18  808338419181031424  Happy Monday NoMa! Getting ready to open our w...   \n",
       "19  806893320588902400  Getting ready to open up our window at Farragu...   \n",
       "\n",
       "                  date  \n",
       "0  2017-01-05 15:27:03  \n",
       "1  2017-01-04 20:52:00  \n",
       "2  2017-01-04 08:47:04  \n",
       "3  2017-01-03 23:46:00  \n",
       "4  2017-01-03 20:52:00  \n",
       "5  2017-01-03 15:30:06  \n",
       "6  2017-01-03 12:36:55  \n",
       "7  2016-12-30 15:27:57  \n",
       "8  2016-12-29 18:38:27  \n",
       "9  2016-12-29 14:22:02  \n",
       "10 2016-12-27 17:28:35  \n",
       "11 2016-12-22 14:06:34  \n",
       "12 2016-12-21 15:46:56  \n",
       "13 2016-12-20 15:53:15  \n",
       "14 2016-12-19 14:47:13  \n",
       "15 2016-12-15 14:50:20  \n",
       "16 2016-12-14 17:08:57  \n",
       "17 2016-12-13 15:08:23  \n",
       "18 2016-12-12 15:51:11  \n",
       "19 2016-12-08 16:08:53  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ArepaZone_merged_arrays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ArepaZone_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResultSet' object has no attribute '_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6966a1707fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#print(ArepaZone_timeline.ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mArepaZone_timeline_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mArepaZone_timeline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mArepaZone_timeline_json_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArepaZone_timeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResultSet' object has no attribute '_json'"
     ]
    }
   ],
   "source": [
    "#Pull in most recent 20 tweets\n",
    "ArepaZone_timeline=api.user_timeline(\"ArepaZone\")\n",
    "#ArepaZone_timeline_json=json.dumps([status._json for status in ArepaZone_timeline])\n",
    "#ArepaZone_timeline_json\n",
    "#with open('ArepaZone_json_output.json', 'w') as outfile:\n",
    "#    json.dump(ArepaZone_timeline_json, outfile)\n",
    "#ArepaZone_json_df = pd.read_json('ArepaZone_json_output.json')\n",
    "#ArepaZone_json_df\n",
    "#print (dir(ArepaZone_timeline))\n",
    "#print(ArepaZone_timeline.ids)\n",
    "ArepaZone_timeline_json=ArepaZone_timeline[0]\n",
    "ArepaZone_timeline_json_str = json.dumps(ArepaZone_timeline._json)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set a number to loop through\n",
    "iterator=-1\n",
    "#Create arrays for each feature of the timeline json object to extract\n",
    "ArepaZone_id_array=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "ArepaZone_date_array=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "ArepaZone_tweet_array=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "#Loop through the tweet objects and add the feature contents to the relevant arrays\n",
    "for tweet in ArepaZone_timeline:\n",
    "    iterator=iterator+1\n",
    "    ArepaZone_id_array[iterator]=tweet.id\n",
    "    ArepaZone_date_array[iterator]=tweet.created_at\n",
    "    ArepaZone_tweet_array[iterator]=tweet.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the pandas dataframe using the ID array to merge the other features into\n",
    "ArepaZone_merged_arrays_df = pd.DataFrame(data=ArepaZone_id_array, columns=['id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dataframes for the other features so they are able to be merged\n",
    "ArepaZone_tweet_array_df = pd.DataFrame(data=ArepaZone_tweet_array, columns=['tweet']) \n",
    "ArepaZone_date_array_df = pd.DataFrame(data=ArepaZone_date_array, columns=['date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge the other feature dataframes into the main dataframe\n",
    "ArepaZone_merged_arrays_df=ArepaZone_merged_arrays_df.merge(ArepaZone_tweet_array_df,left_index=True, right_index=True)\n",
    "ArepaZone_merged_arrays_df=ArepaZone_merged_arrays_df.merge(ArepaZone_date_array_df,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Output the dataframe to a text file to save it (will eventually want to do this in SQL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ArepaZone_merged_arrays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These are functions of all of these commands above\n",
    "def tweets_to_dataframe(food_truck_twitter_handle, food_truck_twitter_handle_string):\n",
    "    food_truck_twitter_handle_timeline=\"\"\n",
    "    food_truck_twitter_handle_timeline=api.user_timeline('{}'.format(food_truck_twitter_handle_string))\n",
    "    #Set a number to loop through\n",
    "    #Create lists\n",
    "    food_truck_twitter_handle_id_array=[0]\n",
    "    food_truck_twitter_handle_date_array=[0]\n",
    "    food_truck_twitter_handle_tweet_array=[\"\"]\n",
    "    #Create arrays for each feature of the timeline json object to extract\n",
    "    #food_truck_twitter_handle_id_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle_string)),dtype=object)\n",
    "    #food_truck_twitter_handle_date_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle_string)),dtype=object)\n",
    "    #food_truck_twitter_handle_tweet_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle_string)),dtype=object)\n",
    "    #Loop through the tweet objects and add the feature contents to the relevant arrays\n",
    "    #iterator=len('{}_timeline'.format(food_truck_twitter_handle_string))-len('{}_timeline'.format(food_truck_twitter_handle_string))\n",
    "    for tweet in food_truck_twitter_handle_timeline:\n",
    "        food_truck_twitter_handle_id_array=food_truck_twitter_handle_id_array.append(getattr(tweet, 'id', None))\n",
    "        #food_truck_twitter_handle_date_array[iterator]=getattr(tweet, 'created_at', None)\n",
    "        #food_truck_twitter_handle_tweet_array[iterator]=getattr(tweet, 'text', None)\n",
    "        #iterator=iterator+1\n",
    "    #Create the pandas dataframe using the ID array to merge the other features into\n",
    "    food_truck_twitter_handle_merged_arrays_df = pd.DataFrame(data=food_truck_twitter_handle_id_array, columns=['id']) \n",
    "    #food_truck_twitter_handle_merged_arrays_df = pd.DataFrame(data=eval('{}_id_array'.format(food_truck_twitter_handle_string)), columns=['id']) \n",
    "    #Create dataframes for the other features so they are able to be merged\n",
    "    #food_truck_twitter_handle_tweet_array_df = pd.DataFrame(data=food_truck_twitter_handle_tweet_array, columns=['tweet']) \n",
    "    #food_truck_twitter_handle_date_array_df = pd.DataFrame(data=food_truck_twitter_handle_date_array, columns=['date']) \n",
    "    #Merge the other feature dataframes into the main dataframe\n",
    "    #food_truck_twitter_handle_merged_arrays_df=food_truck_twitter_handle_merged_arrays_df.merge(food_truck_twitter_handle_tweet_array_df,left_index=True, right_index=True)\n",
    "    #food_truck_twitter_handle_merged_arrays_df=food_truck_twitter_handle_merged_arrays_df.merge(food_truck_twitter_handle_date_array_df,left_index=True, right_index=True)\n",
    "    log_tweets = \"/Users/nicolekelly/Documents/ft_aggregate/\" + str(time.time()) + '_' + '{}'.format(food_truck_twitter_handle_string)+'.csv'\n",
    "    food_truck_twitter_handle_merged_arrays_df.to_csv(path_or_buf=log_tweets, header=['ID'], index=True, sep=',')\n",
    "    #food_truck_twitter_handle_merged_arrays_df.to_csv(path_or_buf=log_tweets, header=['ID','Tweet','Date'], index=True, sep=',')\n",
    "    return food_truck_twitter_handle_merged_arrays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These are functions of all of these commands above\n",
    "#trucks=['ArepaZone','abunaifood', 'healthyfool', 'theballtruck']\n",
    "trucks=['captaincookiedc']\n",
    "for food_truck_twitter_handle in trucks:\n",
    "    food_truck_twitter_handle_timeline=api.user_timeline('{}'.format(food_truck_twitter_handle))\n",
    "    #Create arrays for each feature of the timeline json object to extract\n",
    "    food_truck_twitter_handle_id_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle)),dtype=object)\n",
    "    food_truck_twitter_handle_date_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle)),dtype=object)\n",
    "    food_truck_twitter_handle_tweet_array=numpy.empty(len('{}_timeline'.format(food_truck_twitter_handle)),dtype=object)\n",
    "    #Loop through the tweet objects and add the feature contents to the relevant arrays\n",
    "    iterator=len('{}_timeline'.format(food_truck_twitter_handle))-len('{}_timeline'.format(food_truck_twitter_handle))\n",
    "    for tweet in food_truck_twitter_handle_timeline:\n",
    "        if getattr(tweet, 'id', None) != None \n",
    "        food_truck_twitter_handle_id_array[iterator]=getattr(tweet, 'id', None)\n",
    "        food_truck_twitter_handle_date_array[iterator]=getattr(tweet, 'created_at', None)\n",
    "        food_truck_twitter_handle_tweet_array[iterator]=getattr(tweet, 'text', None)\n",
    "        iterator=iterator+1\n",
    "    #Create the pandas dataframe using the ID array to merge the other features into\n",
    "    food_truck_twitter_handle_merged_arrays_df = pd.DataFrame(data=food_truck_twitter_handle_id_array, columns=['id']) \n",
    "    #food_truck_twitter_handle_merged_arrays_df = pd.DataFrame(data=eval('{}_id_array'.format(food_truck_twitter_handle_string)), columns=['id']) \n",
    "    #Create dataframes for the other features so they are able to be merged\n",
    "    food_truck_twitter_handle_tweet_array_df = pd.DataFrame(data=food_truck_twitter_handle_tweet_array, columns=['tweet']) \n",
    "    food_truck_twitter_handle_date_array_df = pd.DataFrame(data=food_truck_twitter_handle_date_array, columns=['date']) \n",
    "    #Merge the other feature dataframes into the main dataframe\n",
    "    food_truck_twitter_handle_merged_arrays_df=food_truck_twitter_handle_merged_arrays_df.merge(food_truck_twitter_handle_tweet_array_df,left_index=True, right_index=True)\n",
    "    food_truck_twitter_handle_merged_arrays_df=food_truck_twitter_handle_merged_arrays_df.merge(food_truck_twitter_handle_date_array_df,left_index=True, right_index=True)\n",
    "    log_tweets = \"/Users/nicolekelly/Documents/ft_aggregate/\" + str(time.time()) + '_' + '{}'.format(food_truck_twitter_handle)+'.csv'\n",
    "    #food_truck_twitter_handle_merged_arrays_df.to_csv(path_or_buf=log_tweets, header=['ID'], index=True, sep=',')\n",
    "    food_truck_twitter_handle_merged_arrays_df.to_csv(path_or_buf=log_tweets, header=['ID','Tweet','Date'], index=True, sep=',')\n",
    "    #return food_truck_twitter_handle_merged_arrays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food_truck_twitter_handle_id_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "healthyfool_merged_arrays_df=tweets_to_dataframe('healthyfool', 'healthyfool')\n",
    "abunai_merged_arrays_df=tweets_to_dataframe('abunaifood', 'abunaifood')\n",
    "ballornothing_merged_arrays_df=tweets_to_dataframe('theballtruck', 'theballtruck')\n",
    "arepazone_merged_arrays_df=tweets_to_dataframe('ArepaZone','ArepaZone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abunai_merged_arrays_df=tweets_to_dataframe('abunaifood', 'abunaifood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "healthyfool_merged_arrays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test code, no longer in use\n",
    "ArepaZone_timeline=api.user_timeline(\"ArepaZone\")\n",
    "#type(ArepaZone_timeline)\n",
    "json_str=json.dumps([status._json for status in ArepaZone_timeline])\n",
    "parsed_json=json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(parsed_json)\n",
    "print(dir(parsed_json))\n",
    "#print(parsed_json['text'])\n",
    "parsed_json.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-c6b8fb03b4ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(tweet[('text','id','created_at')])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0marepa_zone_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'map'"
     ]
    }
   ],
   "source": [
    "new_array=numpy.empty([20,3], dtype=str)\n",
    "for tweet in parsed_json:\n",
    "    #print(dir(tweet))\n",
    "    #print(tweet[('text','id','created_at')])\n",
    "    counter=len(parsed_json)-len(parsed_json)\n",
    "    arepa_zone_array[counter]=map(tweet.get, ['text','id','created_at'])\n",
    "    counter=counter+1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "arepa_zone_list\n",
    "len(parsed_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#User Timeline function brings in all sorts of information about the account/tweet\n",
    "#I only care about the text string\n",
    "#Regular expression that captures section that starts with \"text:\"\n",
    "#I might also want dates soon too\n",
    "#regex_screenname_id = r\"\\\"contributors_enabled\\\"\\:\\s\\w+\\,\\s\\\"id\\\"\\:\\s(\\w+)\"\n",
    "#regex_screenname = r\"\\\"screen_name\\\"\\:\\s\\\"(.*?)\\\"\"\n",
    "#regex_tweet_id = r\"\\\"id\\\"\\:\\s(\\w+)\\,\\s\\\"text\"\n",
    "#regex_date = r\"\\\"created_at\\\"\\:\\s\\\"(.*?)\\\"\"\n",
    "#regex_text = r\"\\\"text\\\"\\:\\s\\\"(.*?)\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function that pulls in the most recent 20 tweets (I'm not sure if I can grab more than 20 right now)\n",
    "#def getTimeline(foodtruckname, foodtruckname_str):\n",
    "#    foodtruckname_timeline=\"\"\n",
    "#    foodtruckname_timeline=api.user_timeline('{}'.format(foodtruckname_str))\n",
    "#    foodtruckname_json_str=json.dumps([status._json for status in foodtruckname_timeline])\n",
    "#    foodtruckname_screenname_id = re.findall(regex_screenname_id, foodtruckname_json_str, flags=0)\n",
    "#    foodtruckname_screenname = re.findall(regex_screenname, foodtruckname_json_str, flags=0)\n",
    "#    foodtruckname_tweet_id = re.findall(regex_tweet_id, foodtruckname_json_str, flags=0)\n",
    "#    foodtruckname_date = re.findall(regex_date, foodtruckname_json_str, flags=0)\n",
    "#    foodtruckname_text = re.findall(regex_text, foodtruckname_json_str, flags=0)\n",
    "#    return {'foodtruckname_screenname_id':foodtruckname_screenname_id, 'foodtruckname_screenname': foodtruckname_screenname,\n",
    "#            'foodtruckname_tweet_id':foodtruckname_tweet_id, 'foodtruckname_date':foodtruckname_date, \n",
    "#            'foodtruckname_text':foodtruckname_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ArepaZone_tweets=getTimeline('ArepaZone', 'ArepaZone')\n",
    "#ArepaZone_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ArepaZone_df=pd.DataFrame.from_dict(ArepaZone_tweets,orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tweet_data = []\n",
    "    # You will use this line in production instead of this\n",
    "    # current_working_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "#current_working_dir = \"./\"\n",
    "#log_tweets = current_working_dir  + str(time.time()) + '_fttweets.txt'\n",
    "#with open(log_tweets, 'w') as outfile:\n",
    "#    for tweet in ArepaZone_timeline:\n",
    "#        tweet_data.append(json.loads(json.dumps(tweet._json)))\n",
    "#        outfile.write(json.dumps(tweet._json))\n",
    "#        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the dataframe we will use\n",
    "#tweets = pd.DataFrame()\n",
    "# We want to know when a tweet was sent\n",
    "#tweets['created_at'] = map(lambda tweet: time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')), tweet_data)\n",
    "# Who is the tweet owner\n",
    "#tweets['user'] = map(lambda tweet: tweet['user']['screen_name'], tweet_data)\n",
    "# How many follower this user has\n",
    "#tweets['user_followers_count'] = map(lambda tweet: tweet['user']['followers_count'], tweet_data)\n",
    "# What is the tweet's content\n",
    "#tweets['text'] = map(lambda tweet: tweet['text'].encode('utf-8'), tweet_data)\n",
    "# If available what is the language the tweet is written in\n",
    "#tweets['lang'] = map(lambda tweet: tweet['lang'], tweet_data)\n",
    "# If available, where was the tweet sent from ?\n",
    "#tweets['Location'] = map(lambda tweet: tweet['place']['country'] if tweet['place'] != None else None, tweet_data)\n",
    "# How many times this tweet was retweeted and favorited\n",
    "#tweets['retweet_count'] = map(lambda tweet: tweet['retweet_count'], tweet_data)\n",
    "#tweets['favorite_count'] = map(lambda tweet: tweet['favorite_count'], tweet_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tweets_raw_data = []\n",
    "i=-1\n",
    "#index=range(len(ArepaZone_timeline))\n",
    "#ArepaZone_id=pd.DataFrame(index=index)\n",
    "ArepaZone_id=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "ArepaZone_date=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "ArepaZone_tweets=numpy.empty(len(ArepaZone_timeline),dtype=object)\n",
    "for tweet in ArepaZone_timeline:\n",
    "    i=i+1\n",
    "    ArepaZone_tweets[i]=tweet.text\n",
    "    ArepaZone_id[i]=tweet.id\n",
    "    ArepaZone_date[i]=tweet.created_at\n",
    "    #print(tweet.text)\n",
    "    #tweets_raw_data.append(tweet)\n",
    "    # Create the dataframe we will use\n",
    "#tweets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ArepaZone_timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ArepaZone_timeline\n",
    "ArepaZone_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ArepaZone_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the dataframe we will use\n",
    "ArepaZone_tweetDF = pd.DataFrame(data=ArepaZone_id, columns=['id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ArepaZone_tweetsDF = pd.DataFrame(data=ArepaZone_tweets, columns=['tweet']) \n",
    "ArepaZone_datesDF = pd.DataFrame(data=ArepaZone_date, columns=['date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ArepaZone_tweetDF=ArepaZone_tweetDF.merge(ArepaZone_tweetsDF,left_index=True, right_index=True)\n",
    "ArepaZone_tweetDF=ArepaZone_tweetDF.merge(ArepaZone_datesDF, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ArepaZone_tweetDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pull in most recent 20 tweets\n",
    "abunai_timeline=api.user_timeline(\"abunaifood\")\n",
    "abunai_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set a number to loop through\n",
    "i=-1\n",
    "#Create arrays for each feature of the timeline json object to extract\n",
    "abunai_id_array=numpy.empty(len(abunai_timeline),dtype=object)\n",
    "abunai_date_array=numpy.empty(len(abunai_timeline),dtype=object)\n",
    "abunai_tweet_array=numpy.empty(len(abunai_timeline),dtype=object)\n",
    "#Loop through the tweet objects and add the feature contents to the relevant arrays\n",
    "for tweet in abunai_timeline:\n",
    "    i=i+1\n",
    "    abunai_id_array[i]=getattr(tweet, 'id', None)\n",
    "    abunai_date_array[i]=tweet.created_at\n",
    "    abunai_tweet_array[i]=tweet.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the pandas dataframe using the ID array to merge the other features into\n",
    "abunai_merged_arrays_df = pd.DataFrame(data=abunai_id_array, columns=['id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dataframes for the other features so they are able to be merged\n",
    "abunai_tweet_array_df = pd.DataFrame(data=abunai_tweet_array, columns=['tweet']) \n",
    "abunai_date_array_df = pd.DataFrame(data=abunai_date_array, columns=['date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge the other feature dataframes into the main dataframe\n",
    "abunai_merged_arrays_df=abunai_merged_arrays_df.merge(abunai_tweet_array_df,left_index=True, right_index=True)\n",
    "abunai_merged_arrays_df=abunai_merged_arrays_df.merge(abunai_date_array_df,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abunai_merged_arrays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Status'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c2e87e0b30df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Loop through the tweet objects and add the feature contents to the relevant arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mArepaZone_timeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mArepaZone_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Status'"
     ]
    }
   ],
   "source": [
    "#Trying David's method of creating a dictionary out of the json\n",
    "#Pull in most recent 20 tweets\n",
    "ArepaZone_timeline=api.user_timeline(\"ArepaZone\")\n",
    "#type(ArepaZone_timeline)\n",
    "#Create dictionary\n",
    "ArepaZone_dictionary={}\n",
    "#Loop through the tweet objects and add the feature contents to the relevant arrays\n",
    "for tweet in ArepaZone_timeline:\n",
    "    ArepaZone_dictionary[tweet]={\"id\":tweet.id, \"text\":tweet.text, \"date\":tweet.created_at}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying the above but trying to convert the unhashable status object into a json object\n",
    "json_str=json.dumps([status._json for status in ArepaZone_timeline])\n",
    "#parsed_json=json.loads(json_str)\n",
    "#type(parsed_json)\n",
    "type(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-001d901ae7d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Loop through the tweet objects and add the feature contents to the relevant arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mArepaZone_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Trying David's method of creating a dictionary out of the json\n",
    "#Pull in most recent 20 tweets\n",
    "ArepaZone_timeline=api.user_timeline(\"ArepaZone\")\n",
    "#Create dictionary\n",
    "ArepaZone_dictionary={}\n",
    "#Loop through the tweet objects and add the feature contents to the relevant arrays\n",
    "for tweet in parsed_json:\n",
    "    ArepaZone_dictionary[tweet]={\"id\":tweet.id, \"text\":tweet.text, \"date\":tweet.created_at}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#api.user_timeline() brings in an object called tweepy.models.ResultSet\n",
    "#This can be converted to a str object via json_str=json.dumps([status._json for status in ArepaZone_timeline])\n",
    "#Or then a list object via json.loads(json_str)\n",
    "#what can I do with the string object or the list object\n",
    "dir(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#second method from David\n",
    "#Pull in most recent 20 tweets\n",
    "ArepaZone_timeline=api.user_timeline(\"ArepaZone\")\n",
    "arepa_tweets=[]\n",
    "for tweet in ArepaZone_timeline:\n",
    "    tweet_parts = {\n",
    "        \"id\":tweet.id,\n",
    "        \"created_at\":tweet.created_at,\n",
    "        \"text\":tweet.text\n",
    "    }\n",
    "    arepa_tweets.append(tweet_parts)\n",
    "\n",
    "ArepaZone_df = pd.DataFrame(arepa_tweets)\n",
    "ArepaZone_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ArepaZone_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
